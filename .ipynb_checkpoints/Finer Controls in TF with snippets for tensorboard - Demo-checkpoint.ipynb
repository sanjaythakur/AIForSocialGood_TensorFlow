{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The same old traditional import statements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the hyperparameters and other important constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The MNIST dataset has 10 classes, representing the digits 0 through 9.\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# The MNIST images are always 28x28 pixels.\n",
    "IMAGE_SIZE = 28\n",
    "\n",
    "# Each image is 28 X 28 in dimension. We flatten the image to get 784 features where each feature would correspond to one pixel.\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE\n",
    "\n",
    "# Batch size \n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# Defining the number of units in hidden layers\n",
    "HIDDEN_LAYER_1 = 50\n",
    "HIDDEN_LAYER_2 = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing the MNIST dataset into our program\n",
    "\n",
    "TensorFlow has support for easily accessing the classic datasets that are used to benchmark different domains within AI. We'll use TF to access the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### For understanding the finer controls in TF, we need to understand the mechanism that runs under the hood. Well, what happened in the last demo, TF constructed a highly computationally efficient computation graph on top of which it performs all its calculations. So, essentially TF mechanism follows two steps:\n",
    "\n",
    "###### 1. Building a computational graph\n",
    "###### 2. Running the computational graph\n",
    "\n",
    "######  Letâ€™s take the two different but intertwined processes of computational graph generation and computations separately, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the computational graph\n",
    "Remember those three steps that we talked about a while ago, we'll create the computational graph to accomplish these three tasks to make any ML model (as shown in this image below).\n",
    "![The three abstract steps to create any ML model](./media/Steps.jpg)\n",
    "\n",
    "Before that, let's first create means to insert inputs into our computational graph. Remember, 'PLACEHOLDERS'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Allocating nodes in the computational graph to accept inputs. These are means to insert inputs into our computational graph.\n",
    "\n",
    "images = tf.placeholder(tf.float32, shape=(None, IMAGE_PIXELS))\n",
    "true_labels = tf.placeholder(tf.int32, shape=(None, NUM_CLASSES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we define the portion of the graph to do the INFERENCE\n",
    "\n",
    "![Portion of the graph to do the INFERENCE](./media/Infer.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the first hidden layer\n",
    "with tf.name_scope('first_hidden_layer'):\n",
    "    weights_layer_1 = tf.Variable(tf.truncated_normal([IMAGE_PIXELS, HIDDEN_LAYER_1],\n",
    "                          stddev=1.0 / math.sqrt(float(IMAGE_PIXELS))), name='weights')\n",
    "\n",
    "    biases_layer_1 = tf.Variable(tf.zeros([HIDDEN_LAYER_1]))\n",
    "\n",
    "    hidden_output_1 = tf.nn.relu(tf.matmul(images, weights_layer_1) + biases_layer_1)\n",
    "\n",
    "# Defining the second hidden layer\n",
    "with tf.name_scope('second_hidden_layer'):\n",
    "    weights_layer_2 = tf.Variable(tf.truncated_normal([HIDDEN_LAYER_1, HIDDEN_LAYER_2],\n",
    "                          stddev=1.0 / math.sqrt(float(HIDDEN_LAYER_1))))\n",
    "\n",
    "    biases_layer_2 = tf.Variable(tf.zeros([HIDDEN_LAYER_2]))\n",
    "\n",
    "    hidden_output_2 = tf.nn.relu(tf.matmul(hidden_output_1, weights_layer_2) + biases_layer_2)\n",
    "\n",
    "# Defining the outputs\n",
    "with tf.name_scope('output'):\n",
    "    weights_output = tf.Variable(tf.truncated_normal([HIDDEN_LAYER_2, NUM_CLASSES],\n",
    "                          stddev=1.0 / math.sqrt(float(HIDDEN_LAYER_2))))\n",
    "\n",
    "    biases_output = tf.Variable(tf.zeros([NUM_CLASSES]))\n",
    "\n",
    "    prediction = tf.matmul(hidden_output_2, weights_output) + biases_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we define the portion of the graph to determine how good our current model is or what the LOSS is\n",
    "\n",
    "![Portion of the graph to determine how good our current model is or what the LOSS is](./media/Loss.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluating the loss function\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=true_labels, logits=prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now we would create a node to keep track of the loss with each iteration that can be later leveraged through TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating nodes for summarization that are supposed to be seen through Tensorboard.\n",
    "with tf.name_scope('summary'):\n",
    "    loss_tracker = tf.summary.scalar('Loss', loss)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we define the portion of the graph to UPDATE the parameters so that the future predictions are better than the current one\n",
    "\n",
    "![Portion of the graph to UPDATE the parameters so that the future predictions are better than the current one](./media/Optimize.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Updating the parameters\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.05)\n",
    "training = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing the computations on the computational graph\n",
    "Once we have created our efficient computational graph, its time for us to pass in the inputs and compute those three abstract steps mentioned above.  \n",
    "But before that we'll create a session variable which serves as a handle to access the computational graph. We'll also initialize the variables that we have declared in our computation graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The following lines of code computes those three abstract steps. Look that the value of loss is decreasing with increasing training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at  0  training step is  2.29445\n",
      "Loss at  100  training step is  2.29298\n",
      "Loss at  200  training step is  2.28746\n",
      "Loss at  300  training step is  2.29012\n",
      "Loss at  400  training step is  2.28751\n",
      "Loss at  500  training step is  2.277\n",
      "Loss at  600  training step is  2.2777\n",
      "Loss at  700  training step is  2.2685\n",
      "Loss at  800  training step is  2.26512\n",
      "Loss at  900  training step is  2.25622\n",
      "Loss at  1000  training step is  2.25354\n",
      "Loss at  1100  training step is  2.25338\n",
      "Loss at  1200  training step is  2.26733\n",
      "Loss at  1300  training step is  2.25173\n",
      "Loss at  1400  training step is  2.24467\n",
      "Loss at  1500  training step is  2.25112\n",
      "Loss at  1600  training step is  2.23886\n",
      "Loss at  1700  training step is  2.23713\n",
      "Loss at  1800  training step is  2.20325\n",
      "Loss at  1900  training step is  2.20964\n",
      "The loss incurred on test set is [2.2142332]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # This line tells our program where to save the files that keep track of any information viewable via tensorboard\n",
    "    location_to_store_at = './graphs/'\n",
    "\n",
    "    writer = tf.summary.FileWriter(location_to_store_at, sess.graph)\n",
    "\n",
    "\n",
    "    for train_step in range(2000):\n",
    "        batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "        if train_step % 100 == 0:\n",
    "            loss_summary, loss_value, _ = sess.run([summary_op, loss, training], {images: batch[0], true_labels: batch[1]})\n",
    "            print('Loss at ', str(train_step), ' training step is ', str(loss_value))\n",
    "            \n",
    "            writer.add_summary(loss_summary, global_step=train_step)\n",
    "            \n",
    "    writer.close()\n",
    "    \n",
    "    # Finally we test how good our final model is by checking the loss incurred on the previously unseen data.\n",
    "    print('The loss incurred on test set is ' + str(sess.run([loss], {images:mnist.test.images, true_labels: mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### If you look closely above we are also computing the value of our node that keeps track of our loss to be later used by tensorboard. We take its value too and use a 'writer' object to write it to a file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voila!!\n",
    "## We did it. We got through this. We made it. Yay!! Woo Hoo!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
